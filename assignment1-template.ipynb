{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to machine learning\n",
    "## \\[5XSL0\\] Fundamentals of Machine Learning - Assignment 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell imports the libraries or packages that you can use during this assignment\n",
    "# you are not allowed to import additional libraries or packages\n",
    "from helpers import *\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Important**\n",
    ">\n",
    "> Do not import any other packages or libraries than the ones already provided to you.\n",
    ">\n",
    "> Write your code between the `BEGIN_TODO` and `END_TODO` markers. Do not change these markers.\n",
    ">\n",
    "> Always give derivations in [markdown cells](https://www.earthdatascience.org/courses/intro-to-earth-data-science/open-reproducible-science/jupyter-python/code-markdown-cells-in-jupyter-notebook/).\n",
    ">\n",
    "> Restart your notebook and run all cells before submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this assignment you will learn about the basic tools that are used in the field of machine learning.\n",
    "The exercises will teach you the mathematical concepts involved in so-called neural networks, by applying them on small scale examples.\n",
    "During the exercises you will also learn how design choices will affect the corresponding optimization procedures and how you can properly assess a model's performance.\n",
    "\n",
    "This assignment is split into 3 parts. Part 1 will brush up the mathematical concepts required for solving machine learning problems by solving a linear system of equations. Part 2 will continue with this problem and will apply the learned methodology to a simple linear regression problem. In part 3 we will make the linear regression model more complex by extending the model to be used for linear classification problems. Although this is a group assignment, you are ought to get familiar with all parts of the assignment. Getting a thorough understanding of the material in this assignment will significantly aid your machine learning expertise.\n",
    "\n",
    "### Learning goals\n",
    "After this assignment you can\n",
    "- use gradient descent for solving convex optimization problems;\n",
    "- solve a linear system of equations directly;\n",
    "- solve a linear system of equations using (stochastic) gradient descent;\n",
    "- explain why (stochastic) gradient descent algorithms are favorable when using larger models;\n",
    "- quantify the performance of a model and explain the influence of the learning rate on the training procedure;\n",
    "- use (stochastic) gradient descent to solve linear regression problems;\n",
    "- use (stochastic) gradient descent to solve linear classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Optimization and linear systems of equations\n",
    "In this part we will brush up some mathematical concepts that we will use in the rest of the assignment. We first discuss partial derivatives, gradients, Jacobians and (stochastic) gradient descent and later apply this to a linear system of equations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of optimization is to maximize or minimize a certain function. If we consider the minization problem, we are interested in finding the global minimum $x^\\star$ of a function $f(x)$, such that \n",
    "$$f(x) \\geq f(x^\\star) \\quad \\forall \\quad x $$\n",
    "holds. In other words, the value of $f(x^\\star)$ at the global minimum $x^\\star$ is the lowest value that $f(x)$ can attain for all values of $x$. When there is only one point that attains this minimum, this inequality is strict: $f(x) > f(x^\\star)$.\n",
    "\n",
    "One way of finding the global minimum is by first determining the stationary points of the function: the points where the derivative of that function equals zero, i.e. $\\frac{\\partial f(x)}{\\partial x} = 0$. In these points the tangent line is horizontal and therefore indicates that we are in a local minimum, a local maximum or in a saddle point. Evaluating the second derivate of the function in these points gives us a measure of the curvature of the function and allows us to determine with which type of stationary point we are dealing with.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.1: Minimizing functions (1 point) \n",
    "Consider the function \n",
    "$$ f(x) = \\frac{1}{4}x^4 - 4x^2 + 2. $$\n",
    "Plot this function for $-5\\leq x \\leq 5$ and algebraically determine its stationary points. Write down your derivations in a markdown cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_1_1a] Plot univariate function\n",
    "\n",
    "x = np.arange(-5,6,0.1)\n",
    "\n",
    "f = list(map(lambda x:(1/4)*(x**4)-4*(x**2)+2, x))\n",
    "\n",
    "plt.plot(x,f)\n",
    "plt.show()\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_1_1a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5XSL0_Assignment1_1_1b] Markdown cell with derivations`\n",
    "\n",
    "$$f'(x) = x^3 - 8x$$\n",
    "$$x^3 - 8x = 0$$\n",
    "$$x(x^2-8)=0$$\n",
    "$$x=0 \\vee x=\\sqrt8 \\vee x=-\\sqrt8$$\n",
    "\n",
    "\n",
    "`#// END_TODO [5XSL0_Assignment1_1_1b] `"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### End of exercise 1.1\n",
    "---\n",
    "---\n",
    "In not all circumstances a function submits to a closed-form solution for its stationary points. In these cases we can resort to an iterative procedure for finding a local minimum. Suppose that we are dealing with a convex function, i.e. a function whose curvature is always positive and therefore its derivative is always increasing. If we start at some arbitrary point $x^{(1)}$ and we would like to move towards a local minimum of the function $f(x)$, we can evaluate the derivative in this point. If the value of the derivative in this point $\\frac{\\partial f(x)}{\\partial x}\\lvert_{x=x^{(1)}}$ is positive, then the function is increasing to the right of this point. In order to minimize $f(x)$ we could then take a small step to the left of $x^{(1)}$ to some $x^{(2)}$ and again evaluate the derivative to make another step. This procedure is known as gradient descent and can be represented as \n",
    "$$ x^{(i+1)} = x^{(i)} - \\lambda \\frac{\\partial f(x)}{\\partial x}\\Bigg\\lvert_{x=x^{(i))}}.$$\n",
    "Note that the minus sign makes sure that we move to the left when the derivative is positive and vice versa. The hyperparameter $\\lambda$ specifies our step size, which is also known as the learning rate. The notation $x^{(i)}$ refers to the point at which we start in the $i^\\text{th}$ iteration of the algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.2: Univariate gradient descent (3 points)\n",
    "Consider the function\n",
    "$$ h(x) = x^2 + e^{x}. $$\n",
    "Calculate the partial derivative of $h(x)$ with respect to $x$ in a Markdown cell below and shortly elaborate on why there is no closed-form solution for determining the stationary point. Furthermore, define functions `function_h()`, `derivative_h()`, `gradient_descent()` and `optimize()`, which:\n",
    "- `function_h(x)` - calculates the value of $h(x)$ for some `x`.\n",
    "- `derivative_h(x)` - calculates the derivative of $h(x)$ at some value `x`.\n",
    "- `gradient_descent(x, learning_rate)` - updates the value of `x` given learning rate hyperparameter.\n",
    "- `optimize(x, learning_rate, nr_iterations)` - iteratively updates the value of `x` using gradient descent.\n",
    "\n",
    "Finally, create 3 plots, one showing $h(x)$ for $-3\\leq x\\leq3$, one showing $h(x^{(i)})$ as a function of the number of iterations and one showing the value of $x^{(i)}$ as a function of the number of iterations.\n",
    "\n",
    "Use the following initial values: $x=3$, $\\lambda=0.001$ and 2000 iterations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5XSL0_Assignment1_1_2a] Derive partial derivative and motivate`\n",
    "\n",
    "$$\\frac{\\partial h(x)}{\\partial x}=2x+e^x$$\n",
    "There is no closed form solution since derivating $$e^x$$ \n",
    "`#// END_TODO [5XSL0_Assignment1_1_2a]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_1_2b] Create univariate gradient descent algorithm\n",
    "\n",
    "x = np.arange(-3,4,0.1)\n",
    "#// END_TODO [5XSL0_Assignment1_1_2b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_1_2c] Run algorithm and plot results\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_1_2c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.2\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we have seen that finding the local minimum of a function can be performed by iteratively optimizing the function. It should be noted that this does not give any guarantees that we are in a global minimum, unless the function is convex.\n",
    "\n",
    "In the field of machine learning oftentimes functions (or models) are created with lots of variables. These variables allow for a flexible model for various tasks. For finding a local minimum in these functions, we should take into account all variables. Given some input $x$ and function $f(x, {\\boldsymbol{\\theta}})$, parameterized by the set of parameters $\\boldsymbol{\\theta} = [\\theta_1, \\theta_2, \\ldots, \\theta_M]^\\top$, we are interested optimizing these parameters. For optimizing multiple parameters we can calculate the gradient\n",
    "$$ \\nabla_{\\boldsymbol{\\theta}} f(x, {\\boldsymbol{\\theta}}) = \\begin{bmatrix} \\frac{\\partial f(x,{\\boldsymbol{\\theta}})}{\\partial \\theta_1} \\\\ \\frac{\\partial f(x,{\\boldsymbol{\\theta}})}{\\partial \\theta_2} \\\\ \\vdots \\\\ \\frac{\\partial f(x, {\\boldsymbol{\\theta}})}{\\partial \\theta_M} \\end{bmatrix}$$\n",
    "which represents a vector whose direction specifies the direction of greatest ascent. Minimizing the parameters therefore results in going in the opposite direction using gradient descent as \n",
    "$$ {\\boldsymbol{\\theta}}^{(i+1)} = {\\boldsymbol{\\theta}}^{(i)} - \\lambda \\ \\nabla_{\\boldsymbol{\\theta}} f(x,{\\boldsymbol{\\theta}}^{(i)}).$$\n",
    "As all terms in this equation are vectors (except for the learning rate $\\lambda$), all parameters will get updated simultaneously with the same step size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.3: Multivariate gradient descent (3 points)\n",
    "Consider the 2-dimensional function\n",
    "$$ h(x_1, x_2) = x_1^2 + x_2^2 - x_1x_2. $$\n",
    "Calculate the gradient of $h(x_1, x_2)$ analytically with respect to $\\boldsymbol{x} = [x_1, x_2]^\\top$ in a Markdown cell below. Furthermore, define functions `function_h()`, `gradient_h()`, `gradient_descent()` and `optimize()`, which:\n",
    "- `function_h(x)` - calculates the value of $h(x_1, x_2)$ for some vector `x`$=[x_1,x_2]^\\top$.\n",
    "- `gradient_h(x)` - calculates the gradient of $h(x)$ at some vector `x`.\n",
    "- `gradient_descent(x, learning_rate)` - updates the value of vector `x` given learning rate hyperparameter.\n",
    "- `optimize(x, learning_rate, nr_iterations)` - iteratively updates the values of vector `x` using gradient descent.\n",
    "\n",
    "Finally, create 2 plots, one contour plot showing $h(x_1, x_2)$ for $-3\\leq x_1\\leq3$ and $-3\\leq x_2\\leq3$ with a line representing the trajectory of the different values of $x^{(i)}$ and its final value represented as a red dot, and one plot showing $h(x^{(i)})$ as a function of the number of iterations.\n",
    "\n",
    "Use the following initial values: $x=[1.5, 2.5]$, $\\lambda=0.001$ and 2500 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5XSL0_Assignment1_1_3a] Derive gradient`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5XSL0_Assignment1_1_3a]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_1_3b] Create multivariate gradient descent algorithm\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_1_3b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_1_3c] Run algorithm and plot results\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_1_3c] Run algorithm and plot results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.3\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the most generic functions that we can use are linear functions. Here a (dependent) variable $y$ is modeled as a linear combination of (explanatory) variables ${\\boldsymbol{x}} = [x_1, x_2, \\dots, x_M]^\\top$ as \n",
    "$$y = f({\\boldsymbol{x}}, {\\boldsymbol{\\theta}}) =\\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_M x_M = \\sum_{m=1}^M \\theta_m x_m = {\\boldsymbol{x}}^\\top {\\boldsymbol{\\theta}}.$$\n",
    "In this equation the parameter vector ${\\boldsymbol{\\theta}} = [\\theta_1, \\theta_2, \\ldots, \\theta_M]^\\top$ defines how much each of the entries in the vector ${\\boldsymbol{x}}$ contributes in forming $y$.\n",
    "\n",
    "Suppose that multiple dependent variables ${\\boldsymbol{y}} = [y_1, y_2, \\ldots, y_N]^\\top$ are available with their corresponding explanatory variable vectors $[{\\boldsymbol{x}}_1, {\\boldsymbol{x}}_2, \\ldots, {\\boldsymbol{x}}_N]$. Let us assume that these dependent variables $y_n$ can be modeled using the same linear function as defined in the equation above, each with the same parameter vector ${\\boldsymbol{\\theta}}$. If we define the matrix $X=[{\\boldsymbol{x}}_1, {\\boldsymbol{x}}_2, \\ldots, {\\boldsymbol{x}}_N]^\\top$ we can write all corresponding equations at once as\n",
    "$$X{\\boldsymbol{\\theta}} = {\\boldsymbol{y}}.\\qquad \\textit{(verify this for yourself)}$$\n",
    "This is also called a *system of linear equations*.\n",
    "\n",
    "Under some conditions this system of linear equations can be solved directly for ${\\boldsymbol{\\theta}}$ given some ${\\boldsymbol{y}}$ and ${\\boldsymbol{X}}$. However, direct inversion of $X$ for calculating the solution is not always possible as this matrix is not guaranteed to be square. In other words, the number of parameters ${\\boldsymbol{\\theta}}$ ($M$) might not equal the number of available variables ($N$). Therefore first both sides need to be multiplied with the transpose of $X$ as\n",
    "$$ X^\\top X {\\boldsymbol{\\theta}} = X^\\top {\\boldsymbol{y}}$$\n",
    "from which we can now take the inverse of $X^\\top X$ as this product yield a square matrix to solve this system of linear equations for ${\\boldsymbol{\\theta}}$.\n",
    "\n",
    ">\n",
    "> Hint: In case you want to do a matrix multiplication in Python instead of an element-wise product, use `@` instead of `*`.\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 1.4: Solving systems of linear equations (2 points)\n",
    "Give the analytical solution of the system of linear equations $X{\\boldsymbol{\\theta}} = {\\boldsymbol{y}}$ for ${\\boldsymbol{\\theta}}$ and specify the dimensions of the individual elements in this solutions in a Markdown cell. Secondly write a function `theta = solve_linearsystem(X, y)` that solves the system of linear equations for some matrix of explanatory variables `X` and vector of dependent variables `y` and which returns the parameter vector `theta`. Verify your written function using the available function `X, y, theta = ex11_generate_linearsystem(M, N)` by comparing the provided value of `theta` with the one you calculated. `M` and `N` specify the number of parameters and the number of data instances, respectively.\n",
    "Finally, explain why this approach becomes computationally challenging for a large number of parameters ${\\boldsymbol{\\theta}}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5XSL0_Assignment1_1_4a] Give analytical solution`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5XSL0_Assignment1_1_4a]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_1_4b] Create theta = solve_linearsystem(X, y) function\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_1_4b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_1_4c] Run algorithm\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_1_4c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5XSL0_Assignment1_1_4d] Discuss computational load`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5XSL0_Assignment1_1_4d]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 1.4\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Linear regression problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised machine learning problems all boil down to the following problem statement:\n",
    "\n",
    "*Given some input data $\\boldsymbol{x}$ and output data $\\boldsymbol{y}$ of some unkown process, that relates these two variables using a (non-)linear function ${\\boldsymbol{y}}=f({\\boldsymbol{x}})$, approximate this input-output relationship by some parameterized model $g_{\\boldsymbol{\\theta}}({\\boldsymbol{x}}) \\approx f({\\boldsymbol{x}})$.*\n",
    "\n",
    "Examples of these problems are:\n",
    "- Given some housing prices (the output data) and some specifications of these houses (the input data), such as the number of rooms, create a model that can predict the housing prices based on a list of specifications.\n",
    "- Given some production information about movies (the input data) and the corresponding return of investments (the output data), determine what aspect of the movie production has the greatest effect on the return of investment. \n",
    "- Given some images of animals (the input data) and a label specifying the animal (the output data), create a model that can recognize which animals are shown on the images.\n",
    "- Given some segment of speech (the input data) and the commands that are being spoken (the output data), create a model which will recognize and execute spoken commands.\n",
    "\n",
    "In the above examples, the input-output relationship is usually not straightforward. Telling a computer how to relate an image (a matrix of numbers) to an animal is extremely difficult to do manually. Therefore, we need to resort to machine learning. We will model the true input-output relationship by some (non-)linear function $g_{\\boldsymbol{\\theta}}({\\boldsymbol{x}})$, which depends on the parameters ${\\boldsymbol{\\theta}}$. The behaviour and so-called input-output mapping of this function $g_{\\boldsymbol{\\theta}}({\\boldsymbol{x}})$ can be very flexible, because there are an infinite ways to tune the parameters ${\\boldsymbol{\\theta}}$, each leading to a different behaviour of $g_{\\boldsymbol{\\theta}}({\\boldsymbol{x}})$. By providing this model with examples of this input-output mapping and by optimizing the parameters ${\\boldsymbol{\\theta}}$, such that this mapping is best predicted, the model can *learn* to relate the input to the output.\n",
    "\n",
    "In the above examples, a distinction can be made between two types of problems. The first two examples relate to regression problems, in which we want to map some input onto a continuous range of values. The final two example are typical examples of classfication problems, in which an input is transformed into a fixed set of values or classes. In the rest of this assignment both problems will be covered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of linear regression is to find a linear relationship between input and output variables. Again consider the situation of Part 1, with the output variables ${\\boldsymbol{y}}$ which are modeled as a linear combination of input variables $X=[{\\boldsymbol{x}}_1, {\\boldsymbol{x}}_2, \\ldots, {\\boldsymbol{x}}_N]^\\top$ as \n",
    "$${\\boldsymbol{y}} \\approx X {\\boldsymbol{\\theta}},$$\n",
    "with $M$ parameters $\\boldsymbol{\\theta}\\in\\mathbb{R}^M$. Here our model creates predictions $\\hat{{\\boldsymbol{y}}} = X\\boldsymbol{\\theta}$, with which we try to approximate the true values of ${\\boldsymbol{y}}$. Linear regression tries to find the optimal value of $\\theta$ such that the predicted values $\\hat{{\\boldsymbol{y}}}$ approximates the true values ${\\boldsymbol{y}}$ best. In order to quantify how good this approximation is, we will define a so-called cost function $\\mathcal{L}({\\boldsymbol{y}}, \\hat{{\\boldsymbol{y}}})$, which tells us how good our model fits our data. In the current example we will use the mean squared error\n",
    "$$ \\mathcal{L}({\\boldsymbol{y}}, \\hat{{\\boldsymbol{y}}}) = \\frac{1}{N} \\sum_{n=1}^N (y_n - \\hat{y}_n)^2 = \\frac{1}{N} \\| {\\boldsymbol{y}} - \\hat{{\\boldsymbol{y}}}\\|^2,$$\n",
    "where the subsript $n$ denotes the data sample index. In other words we average over all squared approximation errors. This averaging allows us to compare performance accross data sets which do not have the same number of data instances.\n",
    "Note that this function is always positive and that this function obtains its minimum when all the predicted values match their true values, i.e. $y_n=\\hat{y}_n \\ \\forall \\ n$.\n",
    "\n",
    "As we have seen in Part 1, direct optimization can be undesirable and therefore we will resort to gradient descent. We would like to optimize the parameters ${\\boldsymbol{\\theta}}$ such that the cost function $\\mathcal{L}({\\boldsymbol{y}}, \\hat{{\\boldsymbol{y}}})$ is minimized and our data is best explained. For the gradient descent algorithm we are first interested in calculating\n",
    "$$ \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}({\\boldsymbol{y}}, \\hat{{\\boldsymbol{y}}}).$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> ### Calculus refresher\n",
    "> Calculating $\\nabla_\\theta J(\\boldsymbol{y}, \\hat{\\boldsymbol{y}})$ is more difficult than in Part 1 of this assignment. The reason for this is that we are now dealing with nested functions.\n",
    "> \n",
    "> From basic calculus you should be aware of the chain rule, which allows us to calculate derivates of nested functions.\n",
    "> The univariate case is simple. If we have the nested function $h(x) = f(g(x))$, with $f$ and $g$ being univariate functions $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ and $g: \\mathbb{R} \\rightarrow \\mathbb{R}$, the derivative of $h$ with respect to $x$ can be written as\n",
    "> $$ \\frac{\\partial h}{\\partial x} = \\frac{\\partial h}{\\partial g} \\frac{\\partial g}{\\partial x}. $$\n",
    "> \n",
    "> This chain rule can be generalized for multivariate functions using Jacobian matrices. A Jacobian matrix is a matrix containing all partial derivates between all inputs and outputs of some function. Consequently, the gradient vector is a specific type of Jacobian matrix, where we have multiple inputs and a single output.\n",
    "> As an example consider the multivariate function $f: \\mathbb{R}^M \\rightarrow \\mathbb{R}^N$. Its Jacobian matrix of size $(N\\times M)$ is defined as\n",
    "> $$J_f(\\boldsymbol{x}) = \\begin{bmatrix} \\nabla_x^\\top f_1 \\\\ \\vdots \\\\ \\nabla_x^\\top f_N \\end{bmatrix}= \\begin{bmatrix}\\frac{\\partial f_1}{\\partial x_1} & \\dots & \\frac{\\partial f_1}{\\partial x_M} \\\\ \\vdots & \\ddots & \\vdots \\\\ \\frac{\\partial f_N}{\\partial x_1} & \\dots & \\frac{\\partial f_N}{\\partial x_M} \\end{bmatrix},$$\n",
    "> where $f_n$ denotes the $n^\\text{th}$ output of $f$.\n",
    "> Using Jacobians, we can define the chain rule of nested multivariate functions. If we have the multivariate nested function $h(\\boldsymbol{x}) = f(g(\\boldsymbol{x})))$, with $f$ and $g$ being multivariate functions $f: \\mathbb{R}^M \\rightarrow \\mathbb{R}^N$ and $g: \\mathbb{R}^K \\rightarrow \\mathbb{R}^M$, the derivative of $h$ with respect to $x$ can be written as\n",
    "> $$ J_h(\\boldsymbol{x}) = J_f(g(\\boldsymbol{x})) J_g(\\boldsymbol{x}),$$\n",
    "> where the individual Jacobian matrices are evaluated at the (intermediate) input of these functions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.1: Linear regression (3 points)\n",
    "Calculate the entries of the analytical Jacobians/gradients $ \\frac{\\partial \\mathcal{L}({\\boldsymbol{y}}, \\hat{\\boldsymbol{y}})}{\\partial \\hat{y}_n}$ and $\\frac{\\partial \\hat{y}_n}{\\partial \\theta_m}$ and based on the obtained results express the corresponding Jacobian matrices (which might be vectors) in terms of $\\boldsymbol{y}$, $\\hat{\\boldsymbol{y}}$ and $X$. You can refer to these Jacobian matrices for the loss function and linear transformation as $J_\\mathcal{L}$ and $J_X$, respectively. Make sure that the dimensions of the Jacobian matrices are correct. Then, use the chain rule for multivariate derivatives to express the total Jacobian matrix $J_\\text{tot}$ of the linear transformation and loss function and explain how it relates to the Jacobian vector $ \\nabla_\\theta \\mathcal{L}({\\boldsymbol{y}}, \\hat{{\\boldsymbol{y}}})$.\n",
    "\n",
    "Use the function `X, y = ex121_generate_data()` to generate some univariate data (i.e. 1-dimensional) and model the data ${\\boldsymbol{y}}$ by a linear model. Use gradient descent for optimizing the parameter(s) $\\boldsymbol{\\theta}$.\n",
    "\n",
    "Finally, make a scatter plot of the data and draw in the same plot a line representing the predictions $\\hat{{\\boldsymbol{y}}}$ using the optimized value of $\\boldsymbol{\\theta}$ for $-5 \\leq x \\leq 5$. Also create a plot showing the cost function over time.\n",
    "\n",
    "Use the following initial values: $\\boldsymbol{\\theta}=[0.0]$, $\\lambda=0.0001$ and 2000 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5XSL0_Assignment1_2_1a] Derivations linear regression`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5XSL0_Assignment1_2_1a]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_2_1b] Gradient descent for linear regression\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_2_1b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_2_1c] Plot results linear regression\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_2_1c]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.1\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above exercise the obtained model does not provide a good fit to the data. From observing the data it seems that there is an offset in the data. The actual underlying model of the data does not seem to cross the point (0, 0), but instead the point (0, 0.5). In order to cope with this, we need to add a so-called DC or bias term $b$ to our model. This requires us to write our linear model as \n",
    "$$y \\approx \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_M x_M + b= \\sum_{m=1}^M \\theta_m x_m + b.$$\n",
    "This bias term can be incorporated in our current model by augmenting our input and parameter vector. If we add to all input data vector an entry of $1$ as $\\tilde{{\\boldsymbol{x}}} = [x_1, x_2, \\ldots, x_M, 1]^\\top$ and if we add the bias term $b$ to our parameter vector as $\\tilde{\\boldsymbol{\\theta}} = [\\boldsymbol{\\theta}^\\top, b]^\\top$, the linear system can be written as \n",
    "$$y \\approx \\tilde{\\boldsymbol{x}}^\\top \\tilde{\\boldsymbol{\\theta}} = \\begin{bmatrix} {\\boldsymbol{x}}^\\top & 1 \\end{bmatrix}\\begin{bmatrix} \\boldsymbol{\\theta} \\\\ b\\end{bmatrix}.$$\n",
    "When dealing with multiple data instances (${\\boldsymbol{y}} \\approx X\\boldsymbol{\\theta}$) we can include this bias term by again adding this term to the parameter vector and by adding a column of ones to $X$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.2: Univariate linear regression with bias (2 points)\n",
    "This exercise is similar to exercise 2.1, but now a bias term will have to added in the linear model.\n",
    "\n",
    "Use the function `X, y = ex121_generate_data()` to generate some univariate data (i.e. 1-dimensional) and model the data ${\\boldsymbol{y}}$ by a linear model with bias term. First augment the input data matrix to include a bias term and then use gradient descent for optimizing the parameter vector $\\tilde{\\boldsymbol{\\theta}}$, which now also includes the bias term.\n",
    "\n",
    "Finally, make a scatter plot of the data and draw in the same plot a line representing the predictions $\\hat{{\\boldsymbol{y}}}$ using the optimized value of $\\tilde{\\boldsymbol{\\theta}}$ for $-5 \\leq x \\leq 5$. Also create a plot showing the cost function over time.\n",
    "\n",
    "Use the following initial values: $\\tilde{\\boldsymbol{\\theta}}=[0.0, 0.0]^\\top$, $\\lambda=0.001$ and 2000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_2_2a] Augment data and perform gradient descent\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_2_2a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_2_2b] Plot results with bias term\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_2_2b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.2\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous exercise we have seen that augmenting the input data matrix $X$ allows us to change our model. Instead of adding a bias term by adding a column of ones, we could also consider manually adding different terms depending on how our data seems to be distributed. If our data seems to be quadratically distribution, a more appropriate model could be \n",
    "$$y \\approx \\theta_1 x_1 + \\theta_2 x_2 + \\ldots + \\theta_M x_M + \\theta_{M+1} x_1^2 + \\theta_{M+2} x_2^2 + \\ldots + \\theta_{2M} x_M^2 + b= \\sum_{m=1}^M \\theta_m x_m + \\sum_{m=M+1}^{2M} \\theta_m x_m^2 + b.$$\n",
    "These quadratic terms can be incorporated in our current model by augmenting our input and parameter vector. We could write this linear system as \n",
    "$$y \\approx \\tilde{\\boldsymbol{x}}^\\top \\tilde{\\boldsymbol{\\theta}} = \\begin{bmatrix} {\\boldsymbol{x}}^\\top & ({\\bf{x}}^{\\circ 2})^\\top & 1 \\end{bmatrix}\\begin{bmatrix} \\boldsymbol{\\theta}_{1:M} \\\\ \\boldsymbol{\\theta}_{M+1:2M} \\\\ b\\end{bmatrix}.$$\n",
    "Here the $\\circ 2$ operations denotes the elementwise power. Note that this model is still linear with respect to its parameters even though it contains quadratic terms.\n",
    "\n",
    "This approach can be extended towards more complicated models. The columns that we are adding in our input data matrix can be considered as hand-crafted features. These features represent our signal in different ways, being just as it is or being quadraticcally multiplied. The hand-crafted aspect states that we have to add these columns ourselves and that we are in control on which features/representations are used in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 2.3: Univariate linear regression with bias and quadratic term (1 point)\n",
    "Use the function `X, y = ex123_generate_data()` to generate some univariate data (i.e. 1-dimensional) and model the data ${\\boldsymbol{y}}$ by a linear model with quadratic and bias terms. First augment the input data matrix to include the quadratic terms and the bias term and then use gradient descent for optimizing the extended parameter vector $\\tilde{\\boldsymbol{\\theta}}$.\n",
    "\n",
    "Finally, make a scatter plot of the data and draw in the same plot a line representing the predictions $\\hat{{\\boldsymbol{y}}}$ using the optimized value of $\\tilde{\\boldsymbol{\\theta}}$ for $-5 \\leq x \\leq 5$. Also create a plot showing the cost function over time.\n",
    "\n",
    "Use the following initial values: $\\tilde{\\boldsymbol{\\theta}}=[0.0, 0.0, 0.0]^\\top$, $\\lambda=0.001$ and 2000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_2_3a] Add quadratic term and perform gradient descent\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_2_3a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_2_3b] Plot results with quadratic term\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_2_3b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 2.3\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Linear classification problems\n",
    "In the linear regression problems that we discussed above the goal was to map some input in $\\mathbb{R}^M$ to some output in $\\mathbb{R}$. Ofcourse this output can also be multidimensional. In linear classification problems the goal is not to map some input to a continuous output, but to map some input into a set of classes. In this assignment we will focus on the case where we have 2 classes. This is also known as a binary or logistic regression problem. For the binary classification task we have some (multidimensional) input data $X$, similarly defined as in the regression problem, and some output ${\\boldsymbol{y}}$, where each of the output $y_n \\in \\{0, 1\\}$. The output value is also known as the label and indicates to which group or cluster a certain point belongs.\n",
    "\n",
    "The linear regression model that we used in Part 2 is not directly suited for these classifications tasks as its output is located in the entire real set. In order to transform this output to an approximation of the set $\\{0, 1\\}$, we need to transform the original output by some non-linear function. A commonly chosen function is the so-called sigmoid function $\\sigma(x)$ which maps the entire real axis to the domain $(0, 1)$. This sigmoid function is defined as \n",
    "$$ \\sigma(x) = \\frac{1}{1+e^{-x}} .$$\n",
    "By extending our previous model with this sigmoid function, our output is now modeled as \n",
    "$$y_n \\approx \\hat{y}_n = \\sigma(z_n) = \\sigma({\\boldsymbol{x}}_n^\\top \\boldsymbol{\\theta}).$$\n",
    "Here $z_n={\\boldsymbol{x}}_n^\\top \\boldsymbol{\\theta}$ is an intermediate variable obtained at the output of the original linear model that we had for the linear regression problem. \n",
    "\n",
    "Through optimization the predicted classes will approximate the true classes as good as possible. The linear model will affinely transform the input data to the real axis with the goal of making the transformed data separable by the sigmoid function. For logistic regression our optimization goal is no longer the mean squared error, but instead we would like to minimize the binary cross-entropy. This cost function is more appropriate as it models the output data as binomial random variables and calculates a probabilistic degree of dissimilarity between the predictions and the true labels. The binary cross-entropy function is defined as \n",
    "$$ \\mathcal{L}({\\boldsymbol{y}}, \\hat{{\\boldsymbol{y}}}) = -\\frac{1}{N} \\sum_{n=1}^N \\left[y_n \\ln (\\hat{y}_n)  + (1-y_n) \\ln (1-\\hat{y}_n) \\right].$$\n",
    "Therefore during optimization the goal is to optimize $\\mathcal{L}({\\boldsymbol{y}}, \\hat{{\\boldsymbol{y}}})$ with respect to $\\boldsymbol{\\theta}$ using \n",
    "$$ \\nabla_{\\boldsymbol{\\theta}} \\mathcal{L}({\\boldsymbol{y}}, \\hat{{\\boldsymbol{y}}}) .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.1: Linear classification preparation (2 points)\n",
    "The goal of this exercise is to prepare you for training a linear classification model.\n",
    "\n",
    "Calculate the entries of the Jacobian matrices $\\frac{\\partial \\mathcal{L}(y_n, \\hat{y}_n)}{\\partial \\hat{y}_n}$ and $\\frac{\\partial \\hat{y}_n}{\\partial z_m}$ analytically. Write down your derivations and results in a Markdown cell. Furthermore describe how the Jacobian matrix of the clasification layer looks like.\n",
    "\n",
    "Furthermore, visualize the data using `X, y = ex13_generate_data()`. The $X$ matrix consists out of a set of 2-dimensional data points and ${\\bf{y}}$ consists out of the corresponding binary labels. Create a scatter plot of the input data and give the two different classes different colors.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`#// BEGIN_TODO [5XSL0_Assignment1_3_1a] Derivations linear classification + question`\n",
    "\n",
    "<div class='alert alert-warning' role='alert'>Replace this line by your text.</div>\n",
    "\n",
    "`#// END_TODO [5XSL0_Assignment1_3_1a]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_3_1b] Plot clusters of data\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_3_1b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.1\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model has been trained, there exist points which are equally likely to be in both classes.  For these points the following holds:\n",
    "$$ \\sigma(z_n) = 0.5 .$$\n",
    "Given the definition of the sigmoid function, we can find that this occurs when $z_n = 0$. As $z_n$ is defined as $z_n = {\\boldsymbol{x}}_n^\\top \\boldsymbol{\\theta}$, we can also write this as \n",
    "$$ {\\boldsymbol{x}}_n^\\top \\boldsymbol{\\theta} = 0.$$ \n",
    "If we include a bias term in our model, the individual vectors are given as ${\\boldsymbol{x}} = [x_1, x_2, 1]^\\top$ and $\\boldsymbol{\\theta} = [\\theta_1, \\theta_2, b]^\\top$. This can be expanded to yield $x_1\\theta_1 + x_2\\theta_2 + b = 0$, from which we can find the representation of a line as \n",
    "$$ x_2 = -\\frac{\\theta_1}{\\theta_2}x_1 - \\frac{b}{\\theta_2},$$\n",
    "with slope $-\\frac{\\theta_1}{\\theta_2}$ and y-axis crossing $-\\frac{b}{\\theta_2}$. This line specifies the points which are equally likely to be assigned to both classes and is also known as the decision boundary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "### Exercise 3.2: Linear classification (4 points)\n",
    "Use the above determined derivative in order to optimize the parameters $\\boldsymbol{\\theta}$ with respect to the data set obtained using `X, y = ex13_generate_data()`. Use the linear model with the sigmoid activation function at its output to minimize the binary cross-entropy. The linear model should also include a bias component. To evaluate the performance, calculate how many percent of the total number of data points is assigned correctly.\n",
    "\n",
    "Create a plot of the cost function as a function of the number of iterations. Furthermore plot the data with the predicted labels and draw the decision boundary. Limit the axis for the data and decision boundary to appropriate values.\n",
    "\n",
    "Use the following hyperparameters: starting point $\\boldsymbol{\\theta} = [0, 0, 0]^\\top$, learning rate $10^{-2}$ and 5000 iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_3_2a] Linear classification using gradient descent algorithm\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_3_2a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#// BEGIN_TODO [5XSL0_Assignment1_3_2b] Plot decision boundary and loss function\n",
    "\n",
    "# ===== =====> Replace this line by your code. <===== ===== #\n",
    "\n",
    "#// END_TODO [5XSL0_Assignment1_3_2b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of exercise 3.2\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    ">   Make sure to restart this notebook and to rerun all cells before submission to check whether all code runs properly."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2400f512a8db153dd44c44e4ced040f982d735ac66a3635132e753707e368f6f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "metadata": {
   "interpreter": {
    "hash": "fbf845ed53a8fe8da30af23ffa617c1e24ca2d627a9cfb72a6271f69c4dfb1ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
